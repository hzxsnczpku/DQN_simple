# DQN_simple

简化了[@smsxgz](https://github.com/smsxgz/oh-my-q-learning)的代码，写了一个简洁的DQN代码，具体的修改如下，

* 去掉了模型存取的模块，反正训练断了之后一般也续不上；
* 去掉了并行的模块，缺点是再也不能并行跑几十万的不科学分数了，但优点是在单进程下速度快了一倍；
* 缩减了几乎所有的代码
* 坚持了用pytorch建模型，用tensorboard存数据的风格，同时在tensorboard上输出数据时不做滑动平均

在Breakout上测试了一下，基本能够很快训出一个好的模型，说明代码并没有被改崩，更多的实验后续慢慢补吧……

以及一点经验：训出一个游戏不靠DQN写得好，全靠环境包装得好……
